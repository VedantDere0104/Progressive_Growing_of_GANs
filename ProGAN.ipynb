{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mo9cS9b60EzbGwRHHTywkoZ-PGNwi9RB",
      "authorship_tag": "ABX9TyNIzzgkp6z2/rWg+FcJMDgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VedantDere0104/Progressive_Growing_of_GANs/blob/main/ProGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLdK-btXL2L3"
      },
      "source": [
        "####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUdOHC7rL7WD"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lDgsA-QNuHu"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Q6ftNgP86r"
      },
      "source": [
        "in_channels_gen = 512\n",
        "out_channels = 3\n",
        "z_dim = 512\n",
        "out_channels_disc = 1\n",
        "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phofXnaAdCgP"
      },
      "source": [
        "class Pixel_Norm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pixel_Norm , self).__init__()\n",
        "\n",
        "    self.epsilon = 1e-8\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = x / torch.sqrt(torch.mean(x ** 2 , dim=1 , keepdim = True) + self.epsilon)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5x6syeudu6M"
      },
      "source": [
        "pixel_norm = Pixel_Norm().to(device)\n",
        "x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "z = pixel_norm(x)\n",
        "torch.max(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jtvOZQBd2rj"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               kernel_size = 3, \n",
        "               stride = 1, \n",
        "               padding = 1 , \n",
        "               gain = 2):\n",
        "    super(Conv , self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels , out_channels , kernel_size , stride , padding)\n",
        "    self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
        "    \n",
        "    self.bias = self.conv1.bias\n",
        "    self.conv1.bias = None\n",
        "\n",
        "    nn.init.normal_(self.conv1.weight)\n",
        "    nn.init.zeros_(self.bias)\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = x * self.scale\n",
        "    x = self.conv1(x)\n",
        "    x = x + self.bias.view(1 , self.bias.shape[0] , 1 , 1)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bp6Opxme98d"
      },
      "source": [
        "conv = Conv(3 , 32).to(device)\n",
        "x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "z = conv(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhg_KDuEfCxy"
      },
      "source": [
        "class ConvT(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels ,\n",
        "               kernel_size = 2 , \n",
        "               stride = 2 , \n",
        "               padding = 0 , \n",
        "               gain = 2):\n",
        "    super(ConvT , self).__init__()\n",
        "\n",
        "    self.convT = nn.ConvTranspose2d(in_channels , out_channels , kernel_size , stride , padding)\n",
        "    self.scale = (gain / (in_channels * (kernel_size ** 2)))**0.5\n",
        "\n",
        "    self.bias = self.convT.bias\n",
        "    self.convT.bias = None\n",
        "\n",
        "    nn.init.normal_(self.convT.weight)\n",
        "    nn.init.zeros_(self.bias)\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = x * self.scale\n",
        "    x = self.convT(x)\n",
        "    x = x + self.bias.view(1 , self.bias.shape[0] , 1 , 1)\n",
        "    return x\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zMxcnq4fh_T"
      },
      "source": [
        "convT = ConvT(3 , 32).to(device)\n",
        "x = torch.randn(2 , 3 , 256 , 256).to(device)\n",
        "z = convT(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue0V-JqohEaR"
      },
      "source": [
        "class Generator_Block(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels ,\n",
        "               out_channels, \n",
        "               kernel_size = 3 ,\n",
        "               stride = 1 , \n",
        "               padding = 1 , \n",
        "               use_norm = True , \n",
        "               use_activation = True):\n",
        "    super(Generator_Block , self).__init__()\n",
        "\n",
        "    self.use_norm = use_norm\n",
        "    self.use_activation = use_activation\n",
        "\n",
        "    self.conv1 = Conv(in_channels , in_channels , kernel_size , stride , padding)\n",
        "    \n",
        "    self.conv2 = Conv(in_channels , out_channels , kernel_size , stride , padding)\n",
        "\n",
        "    if self.use_norm:\n",
        "      self.norm = Pixel_Norm()\n",
        "    if self.use_activation:\n",
        "      self.activation = nn.LeakyReLU(0.2)\n",
        "  \n",
        "  def forward(self , x):\n",
        "    x = self.conv1(x)\n",
        "    if self.use_norm:\n",
        "      x = self.norm(x)\n",
        "    if self.use_activation:\n",
        "      x = self.activation(x)\n",
        "    \n",
        "    x = self.conv2(x)\n",
        "    if self.use_norm:\n",
        "      x = self.norm(x)\n",
        "    if self.use_activation:\n",
        "      x = self.activation(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jbs6llsiKTs"
      },
      "source": [
        "generator_block = Generator_Block(3 , 32).to(device)\n",
        "summary(generator_block , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf-VrlXgiTc5"
      },
      "source": [
        "class Initial_Block(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels):\n",
        "    super(Initial_Block , self).__init__()\n",
        "\n",
        "    self.convT = ConvT(in_channels , out_channels , kernel_size=4 , stride=1 , padding=0)\n",
        "    self.conv = Conv(out_channels , out_channels )\n",
        "    self.lrelu = nn.LeakyReLU(0.2)\n",
        "    self.pixel_norm = Pixel_Norm()\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.lrelu(self.pixel_norm(self.convT(x)))\n",
        "    x = self.lrelu(self.pixel_norm(self.conv(x)))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fdZBmahiaAR"
      },
      "source": [
        "init_block = Initial_Block(512, 512).to(device)\n",
        "summary(init_block , (512 , 1 , 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjsb9Jc0jML-"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels):\n",
        "    super(Generator , self).__init__()\n",
        "\n",
        "    filters = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32 ]\n",
        "    self.initial_block = Initial_Block(in_channels , in_channels)\n",
        "    #self.init_last = Conv(in_channels , out_channels , kernel_size=1 , stride=1 , padding=0)\n",
        "    self.init_last = Generator_Block(in_channels , \n",
        "                                     out_channels , \n",
        "                                     kernel_size=1 , \n",
        "                                     stride=1 , \n",
        "                                     padding=0 , \n",
        "                                     use_norm=False ,\n",
        "                                     use_activation=False)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    self.conv_ = nn.ModuleList()\n",
        "    self.last_ = nn.ModuleList()\n",
        "\n",
        "    for i in range(len(filters) -1):\n",
        "      conv_in_channels = int(in_channels * filters[i])\n",
        "      conv_out_channels = int(in_channels * filters[i + 1])\n",
        "      #print(conv_in_channels , conv_out_channels  , filters[i])\n",
        "      self.conv_.append(Generator_Block(conv_in_channels , conv_out_channels))\n",
        "      self.last_.append(Conv(conv_out_channels , out_channels , kernel_size=1 , stride=1 , padding=0))\n",
        "\n",
        "    self.upsample = nn.Upsample(scale_factor=2)\n",
        "    \n",
        "  def fade_in(self , upscaled , generated , alpha):\n",
        "    return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
        "\n",
        "  def forward(self , x ,alpha ,  steps):\n",
        "    out = self.initial_block(x)\n",
        "    if steps == 0:\n",
        "      out = self.init_last(out)\n",
        "      return self.sigmoid(out)\n",
        "    #print(out.shape)\n",
        "    for step in range(steps):\n",
        "      upscaled = self.upsample(out)\n",
        "      out = self.conv_[step](upscaled)\n",
        "      #print(upscaled.shape , out.shape)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    upscaled = self.last_[step - 1](upscaled)\n",
        "    generated = self.last_[step](out)\n",
        "\n",
        "    z = self.fade_in(upscaled , generated , alpha)\n",
        "    return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K25rsMGgl_uM"
      },
      "source": [
        "generator_ = Generator(512 , 3).to(device)\n",
        "x = torch.randn(2 , 512 , 1 , 1).to(device)\n",
        "alpha = 1\n",
        "steps = 8\n",
        "z = generator_(x , alpha , steps)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Nxygyj8g7s"
      },
      "source": [
        "class Disc_Block(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               kernel_size = 3 , \n",
        "               stride = 1, \n",
        "               padding = 1 , \n",
        "               use_norm = True , \n",
        "               use_activation = True , \n",
        "               use_pool = True , \n",
        "               use_second_conv = True):\n",
        "    super(Disc_Block , self).__init__()\n",
        "\n",
        "    self.use_norm = use_norm\n",
        "    self.use_activation = use_activation\n",
        "    self.use_pool = use_pool\n",
        "    self.use_second_conv = use_second_conv\n",
        "\n",
        "\n",
        "    self.conv1 = Conv(in_channels , out_channels , kernel_size , stride , padding)\n",
        "    \n",
        "    if self.use_norm:\n",
        "      self.norm = nn.InstanceNorm2d(out_channels)\n",
        "      self.norm1 = nn.InstanceNorm2d(out_channels)\n",
        "    if self.use_activation:\n",
        "      self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2 , stride=2)\n",
        "\n",
        "    self.conv2 = Conv(out_channels , out_channels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.conv1(x)\n",
        "    if self.use_norm:\n",
        "      x = self.norm(x)\n",
        "    if self.use_activation:\n",
        "      x = self.activation(x)\n",
        "\n",
        "    if self.use_second_conv:\n",
        "      x = self.conv2(x)\n",
        "      if self.use_norm:\n",
        "        x = self.norm1(x)\n",
        "      if self.use_activation:\n",
        "        x = self.activation(x)\n",
        "  \n",
        "    if self.use_pool:\n",
        "      x = self.pool(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8T4S0Qy9323"
      },
      "source": [
        "disc_block = Disc_Block(32 , 3 , use_second_conv=False).to(device)\n",
        "summary(disc_block , (32, 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE1xRO3lZimi"
      },
      "source": [
        "class To_Discriminator(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels):\n",
        "    super(To_Discriminator , self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(in_channels , in_channels * 2)\n",
        "    self.batchnorm1 = nn.BatchNorm1d(in_channels * 2)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.linear2 = nn.Linear(in_channels * 2 , in_channels * 4)\n",
        "    self.batchnorm2 = nn.BatchNorm1d(in_channels * 4)\n",
        "    \n",
        "    self.linear3= nn.Linear(in_channels * 4 , out_channels)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.relu(self.batchnorm1(self.linear1(x)))\n",
        "    x = self.relu(self.batchnorm2(self.linera2(x)))\n",
        "    x = self.sigmoid(self.linear3)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcEauDxF5rKp"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self , \n",
        "               in_channels , \n",
        "               out_channels , \n",
        "               hidden_dim = 16):\n",
        "    super(Discriminator , self).__init__()\n",
        "\n",
        "    self.init_conv = Disc_Block(in_channels , hidden_dim)\n",
        "    self.init_last = Conv(hidden_dim + 1 , out_channels , kernel_size=2 , stride=2 , padding=0)\n",
        "\n",
        "    self.conv_ = nn.ModuleList()\n",
        "    self.last_ = nn.ModuleList()\n",
        "\n",
        "    filters = [1 , 2 , 4 , 8 , 16 , 32 , 32 , 32 , 32]\n",
        "\n",
        "    for i in range(len(filters)-1):\n",
        "      conv_in_channels = hidden_dim * filters[i]\n",
        "      conv_out_channels = hidden_dim * filters[i+1]\n",
        "      self.conv_.append(Disc_Block(conv_in_channels , conv_out_channels))\n",
        "      self.last_.append(Conv(conv_out_channels + 1 , out_channels , kernel_size=2 , stride=2 , padding=0))\n",
        "\n",
        "    self.last_conv = Disc_Block(conv_out_channels , conv_out_channels)\n",
        "    self.last_layer = Conv(conv_out_channels , out_channels , kernel_size=2 , stride=2 , padding=0)\n",
        "\n",
        "    #print(self.conv_)\n",
        "\n",
        "  def minibatch_std(self , x):\n",
        "    x_ = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "    #print(x_.shape , x.shape)\n",
        "    x = torch.cat([x , x_] , dim=1)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def forward(self  , x , alpha , steps):\n",
        "    out = self.init_conv(x)\n",
        "    if steps == 0:\n",
        "      out = self.minibatch_std(out)\n",
        "      out = self.init_last(out)\n",
        "      return out\n",
        "\n",
        "    #print(out.shape)\n",
        "\n",
        "    for step in range(steps):\n",
        "      #print(f'Step {step}')\n",
        "      out = self.conv_[step](out)\n",
        "      if step + 1 == steps:\n",
        "        out = self.minibatch_std(out)\n",
        "        out = self.last_[step](out)\n",
        "      #print(f'out.shape {out.shape}')\n",
        "\n",
        "    #print(out.shape)\n",
        "\n",
        "    #out = self.last_conv(out)\n",
        "    #out = self.last_layer(out)\n",
        "\n",
        "    return out.view(out.shape[0] , -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbEQOYgP8OdB"
      },
      "source": [
        "disc = Discriminator(3 , 1).to(device)\n",
        "x = torch.randn(2 , 3 , 1024 , 1024).to(device)\n",
        "z = disc(x , 0.5 , 8)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crPWLXJ4hMbf"
      },
      "source": [
        "\n",
        "def show_tensor_images(image_tensor, num_images=2, size=(3 , 1024 , 1024)):\n",
        "  image_shifted = image_tensor\n",
        "  image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
        "  image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "  plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYffNZi-IcLD"
      },
      "source": [
        "def crop(image, new_shape):\n",
        "    middle_height = image.shape[2] // 2\n",
        "    middle_width = image.shape[3] // 2\n",
        "    starting_height = middle_height - new_shape[2] // 2\n",
        "    final_height = starting_height + new_shape[2]\n",
        "    starting_width = middle_width - new_shape[3] // 2\n",
        "    final_width = starting_width + new_shape[3]\n",
        "    cropped_image = image[:, :, starting_height:final_height, starting_width:final_width]\n",
        "    return cropped_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt9IobVaJ0D4"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRdUv8eTId4q"
      },
      "source": [
        "dataset = torchvision.datasets.ImageFolder('/content/drive/MyDrive/Celeb_hq/celeba_hq/train/' , transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rs2F27QJqIP"
      },
      "source": [
        "batch_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROlqj2XFI8Kp"
      },
      "source": [
        "dataloader = DataLoader(dataset , batch_size , shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_jhgmZ6StuV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afE0yKmSSru2"
      },
      "source": [
        "def resize_tensor(input_tensors, h, w):\n",
        "  final_output = None\n",
        "  batch_size, channel, height, width = input_tensors.shape\n",
        "  input_tensors = torch.squeeze(input_tensors, 1)\n",
        "  \n",
        "  for img in input_tensors:\n",
        "    img_PIL = transforms.ToPILImage()(img)\n",
        "    img_PIL = torchvision.transforms.Resize([h,w])(img_PIL)\n",
        "    img_PIL = torchvision.transforms.ToTensor()(img_PIL)\n",
        "    if final_output is None:\n",
        "      final_output = img_PIL\n",
        "    else:\n",
        "      final_output = torch.unsqueeze(final_output , dim=0)\n",
        "      img_PIL = torch.unsqueeze(img_PIL, 0)\n",
        "      #print(final_output.shape , img_PIL.shape)\n",
        "      final_output = torch.cat((final_output, img_PIL), 0)\n",
        "      #print(final_output.shape)\n",
        "  #final_output = torch.unsqueeze(final_output, 1)\n",
        "  #print(final_output.shape)\n",
        "  return final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWzoZ36jJX2I"
      },
      "source": [
        "for x , y in dataloader:\n",
        "  print(x.shape)\n",
        "  show_tensor_images(x , num_images=2)\n",
        "  x = resize_tensor(x , 64 , 64)\n",
        "  print(x.shape)\n",
        "  show_tensor_images(x , size=(3 , 64 , 64))\n",
        "\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1kn0jAONYIk"
      },
      "source": [
        "n_epochs = [2 , 3 , 4 , 5 , 10 , 50 , 100 , 200]\n",
        "display_step = [100 , 75 , 50 , 25 , 10 , 5]\n",
        "batch_size = 2\n",
        "lr = 0.0002\n",
        "target_shape = 512\n",
        "betas = (0.5 , 0.999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Pvmz8CQBr5"
      },
      "source": [
        "progan_steps = [0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHAB_nS1Kpm2"
      },
      "source": [
        "generator = Generator(in_channels_gen , out_channels).to(device)\n",
        "discriminator = Discriminator(out_channels ,out_channels_disc).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4enfdruJwrh"
      },
      "source": [
        "opt_generator = torch.optim.Adam(generator.parameters() , lr=lr , betas=betas)\n",
        "opt_discriminator = torch.optim.Adam(discriminator.parameters() , lr=lr , betas = betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM4HrsUCKbYq"
      },
      "source": [
        "adv_criterion = nn.BCEWithLogitsLoss()\n",
        "recon_criterion = nn.L1Loss()\n",
        "lambda_recon = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCEpV0TsOQVJ"
      },
      "source": [
        "def get_gen_loss( cur_batch_size,\n",
        "                 alpha , \n",
        "                 pro_steps , \n",
        "                 real_img , \n",
        "                 z_dim = z_dim , \n",
        "                 generator = generator , \n",
        "                 discriminator = discriminator , \n",
        "                 adv_criterion = adv_criterion , \n",
        "                 recon_criterion = recon_criterion , \n",
        "                 lambda_recon = lambda_recon):\n",
        "  noise = torch.randn((cur_batch_size , z_dim , 1 , 1) , device = device , requires_grad=True , dtype=torch.float)\n",
        "  fake_img = generator(noise , alpha , pro_steps)\n",
        "  disc_fake_pred = discriminator(fake_img , alpha , pro_steps)\n",
        "  disc_fake_loss = adv_criterion(disc_fake_pred , torch.zeros_like(disc_fake_pred))\n",
        "\n",
        "  #real_img = crop(real_img , fake_img.shape)\n",
        "  real_img = resize_tensor(real_img , fake_img.shape[2] , fake_img.shape[3])\n",
        "  #print(real_img.shape , fake_img.shape)\n",
        "  #show_tensor_images(real_img , size=(3 , fake_img.shape[2] , fake_img.shape[3]))\n",
        "\n",
        "  gen_adv_loss = adv_criterion(fake_img , real_img)\n",
        "  gen_recon_loss = recon_criterion(fake_img , real_img)\n",
        "\n",
        "  loss = disc_fake_loss + lambda_recon * gen_adv_loss + lambda_recon * gen_recon_loss\n",
        "\n",
        "  return loss , real_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TSwsupnPq9F"
      },
      "source": [
        "def train():\n",
        "  mean_generator_loss = 0\n",
        "  mean_discriminator_loss = 0\n",
        "  cur_step = 0\n",
        "  alpha = 1e-5\n",
        "  for pro_step in progan_steps:\n",
        "    for epoch in range(n_epochs[pro_step]):\n",
        "      for real_img , _ in tqdm(dataloader):\n",
        "        real_img = real_img.to(device)\n",
        "        cur_batch_size = real_img.shape[0]\n",
        "        #print(pro_step)\n",
        "        #show_tensor_images(real_img)\n",
        "\n",
        "        opt_generator.zero_grad()\n",
        "        \n",
        "        gen_loss , real_img_ = get_gen_loss(cur_batch_size , alpha , pro_step , real_img)\n",
        "\n",
        "        gen_loss.backward()\n",
        "        opt_generator.step()\n",
        "\n",
        "        opt_discriminator.zero_grad()\n",
        "        noise = torch.randn((cur_batch_size , z_dim , 1 , 1) , requires_grad=True , dtype=torch.float).to(device)\n",
        "        with torch.no_grad():\n",
        "          fake_img = generator(noise , alpha , pro_step)\n",
        "        disc_fake_pred = discriminator(fake_img , alpha , pro_step)\n",
        "        disc_real_pred = discriminator(real_img , alpha , pro_step)\n",
        "\n",
        "        disc_fake_loss = adv_criterion(disc_fake_pred , torch.zeros_like(disc_fake_pred))\n",
        "        disc_real_loss = adv_criterion(disc_real_pred , torch.ones_like(disc_real_pred))\n",
        "\n",
        "        disc_loss = (disc_fake_loss + disc_real_loss)/2\n",
        "\n",
        "        disc_loss.backward()\n",
        "        opt_discriminator.step()\n",
        "\n",
        "        mean_discriminator_loss += disc_loss.item() / display_step[pro_step]\n",
        "        mean_generator_loss += gen_loss.item() / display_step[pro_step]\n",
        "\n",
        "        if cur_step % display_step[pro_step] == 0:\n",
        "          if cur_step > 0:\n",
        "            print(f\"ProGAN Steps {pro_step} :Epoch {epoch}: Step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
        "          else:\n",
        "            print(\"Pretrained initial state\")\n",
        "          print('Real_image')\n",
        "          #print(torch.max(real_img) , torch.min(real_img))\n",
        "          show_tensor_images(real_img)\n",
        "          print('Resized Real_image')\n",
        "          #print(torch.max(real_img_) , torch.min(real_img_))\n",
        "          show_tensor_images(real_img_ , size=(real_img_.shape[1] , real_img_.shape[2] , real_img_.shape[3]))\n",
        "          print('Generated_image')\n",
        "          #print(torch.max(fake_img) , torch.min(fake_img))\n",
        "          show_tensor_images(fake_img , size=(fake_img.shape[1] , fake_img.shape[2] , fake_img.shape[3]))\n",
        "          mean_generator_loss = 0\n",
        "          mean_discriminator_loss = 0\n",
        "        cur_step += 1   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2tVjqEdQnDz"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o55Jx8OPQowZ"
      },
      "source": [
        "torch.save(generator.state_dict() , '/content/drive/MyDrive/Pro_GAN_Generator.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlwD4mwiApUJ"
      },
      "source": [
        "torch.save(discriminator.state_dict() , '/content/drive/MyDrive/Pro_GAN_Discriminator.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq6h2XUNnRod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}